{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a251cced",
   "metadata": {},
   "source": [
    "# torch.use_deterministic_algorithms\n",
    "\n",
    "A simple tool to help write reproducible PyTorch applications.\n",
    "\n",
    "## Why do we care about reproducibility?\n",
    "\n",
    "PyTorch is often used to train and run models to make decisions. Imagine that you create a model that can sometimes make different decisions given the same exact initial state (same platform, evironment, inputs, random seeds, etc.). Furthermore, imagine that each time you train the model with the same set of training data, the trained weights in the model are slightly different each time. In an extreme example of a self-driving car, this extra variability could increase the chances of an accident.\n",
    "\n",
    "Simply put, a reproducible application is a better application. (However, in many cases, the benefits may not outweigh the performance costs. It depends on the application.)\n",
    "\n",
    "## What kinds of algorithms are nondeterministic?\n",
    "\n",
    "There are many examples, but I'll explain one: an algorithm that adds a bunch of floating point numbers together. If the numbers are summed in a different order each time, we can get a different result.\n",
    "\n",
    "Below, we calculate the total sum of all the numbers in `a` and `b` three different ways. Each method gives a slightly different result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03ff4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.100000000000168e-25\n",
      "1.1000000000002118e-25\n",
      "1.0999999999998715e-25\n"
     ]
    }
   ],
   "source": [
    "a = [1e-30] * 10000\n",
    "b = [1e-29] * 10000\n",
    "\n",
    "separate_sum = sum(a) + sum(b)\n",
    "combined_sum = sum(a + b)\n",
    "interleaved_sum = sum([a_el + b_el for a_el, b_el in zip(a, b)])\n",
    "\n",
    "print(separate_sum)\n",
    "print(combined_sum)\n",
    "print(interleaved_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910ab622",
   "metadata": {},
   "source": [
    "One way nondeterministic summing could happen in a real application is if a large calculation is split up into multiple threads. Both the way the work is split up and the way the results are combined at the end could vary if those decisions are left to the system software. For instance, to save time, the system may decide to combine a given thread's result as soon as it is available, rather than prescribing a specific order.\n",
    "\n",
    "\n",
    "## How do we guarantee reproducibility?\n",
    "\n",
    "The pedantic answer is: we actually can't! The only way to be 100% confident that an operation is reproducible is to run it an infinite number of times with every possible input and show that it gives a consistent result every time. Unfortunately, we only have a finite amount of time. \n",
    "\n",
    "Instead of guaranteeing that an operator is deterministic, we can tell users specifically which operators we know are nondeterministic.\n",
    "\n",
    "In PyTorch we know that an operation is nondeterministic if either:\n",
    "* Someone has reported seeing the operation act nondeterministically after following the [Reproducibility guide](https://pytorch.org/docs/master/notes/randomness.html)\n",
    "* We carefully look at the code for an operation and see that it uses an algorithm known to be nondeterminisic\n",
    "\n",
    "## Enter `torch.use_deterministic_algorithms`\n",
    "\n",
    "When this setting is turned on, all operations that use nondeterministic algorithms will either switch to a deterministic implementation or raise an error when they are called.\n",
    "\n",
    "The documentation contains a list of all affected operations and the conditions under which they are affected.\n",
    "\n",
    "[Link to documentation](https://pytorch.org/docs/master/generated/torch.use_deterministic_algorithms.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835254f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://pytorch.org/docs/master/generated/torch.use_deterministic_algorithms.html\" width=\"800\" height=\"500\"></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://pytorch.org/docs/master/generated/torch.use_deterministic_algorithms.html\" width=\"800\" height=\"500\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f27ff74",
   "metadata": {},
   "source": [
    "## Example (raise error)\n",
    "\n",
    "As the above documentation shows, `kthvalue` is one of the operations that throws an error if `use_deterministic_algorithms` is turned on. We can see its nondeterministic behavior below. If the k-th value has duplicates, the index of the result can be any one of the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf54498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value: 0.0, index: 16\n",
      "value: 0.0, index: 80\n",
      "value: 0.0, index: 112\n",
      "value: 0.0, index: 240\n",
      "value: 0.0, index: 80\n",
      "value: 0.0, index: 464\n",
      "value: 0.0, index: 80\n",
      "value: 0.0, index: 80\n",
      "value: 0.0, index: 16\n",
      "value: 0.0, index: 112\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.use_deterministic_algorithms(False) # False is the default state\n",
    "torch.manual_seed(0)\n",
    "\n",
    "a = torch.zeros(10000, device='cuda')\n",
    "\n",
    "for _ in range(10):\n",
    "    # Seed each time, just to show that nondeterminism does not come from RNG\n",
    "    torch.manual_seed(0)\n",
    "    \n",
    "    res = a.kthvalue(1)\n",
    "    \n",
    "    # Index can be different each time\n",
    "    print(f'value: {res.values}, index: {res.indices}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2573961c",
   "metadata": {},
   "source": [
    "If we turn `use_deterministic_algorithms` on, we get an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31d92781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work2/kurtamohler/development/pytorch-det-complex-autograd/torch/__init__.py:470: UserWarning: torch.use_deterministic_algorithms is in beta, and its design and functionality may change in the future. (Triggered internally at  ../aten/src/ATen/Context.cpp:69.)\n",
      "  _C._set_deterministic_algorithms(mode)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "kthvalue CUDA does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-164057149d97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_deterministic_algorithms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkthvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: kthvalue CUDA does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True)'. You can turn off determinism just for this operation if that's acceptable for your application. You can also file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation."
     ]
    }
   ],
   "source": [
    "torch.use_deterministic_algorithms(True)\n",
    "a.kthvalue(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b080ac39",
   "metadata": {},
   "source": [
    "## Example (run alternate deterministic algorithm)\n",
    "\n",
    "`torch.bmm` is one of the operations that runs an alternate deterministic algorithm when `use_deterministic_algorithms` is turned on. Below, we have a test that exercises the nondeterministic behavior multiple times and compares the results each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd5a64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bmm_test(num_iters=100):\n",
    "    first_res = None\n",
    "\n",
    "    for iter_num in range(num_iters):\n",
    "        # Seed RNG to ensure we have the same data each time\n",
    "        torch.manual_seed(0)\n",
    "\n",
    "        a = torch.randn(100, 100, 100, device='cuda').to_sparse()\n",
    "        b = torch.randn(100, 100, 100, device='cuda')\n",
    "\n",
    "        res = torch.bmm(a, b)\n",
    "\n",
    "        if first_res is None:\n",
    "            first_res = res.clone()\n",
    "        else:\n",
    "            if not res.eq(first_res).all():\n",
    "                print(f'Result mismatch after iteration {iter_num}')\n",
    "                return\n",
    "    print(f'Results match after all {num_iters} iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5151dc",
   "metadata": {},
   "source": [
    "When `use_deterministic_algorithms` is turned off, we get a mismatch after only the second iteration of the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1045634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result mismatch after iteration 1\n"
     ]
    }
   ],
   "source": [
    "torch.use_deterministic_algorithms(False)\n",
    "run_bmm_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5cb37b",
   "metadata": {},
   "source": [
    "But when it's turned on, results match each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e5bc8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results match after all 100 iterations\n"
     ]
    }
   ],
   "source": [
    "torch.use_deterministic_algorithms(True)\n",
    "run_bmm_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b67c41",
   "metadata": {},
   "source": [
    "## How does `torch.use_deterministic_algorithms` detect nondeterminism?\n",
    "\n",
    "Nothing fancy. We just hard code it into each operation.\n",
    "\n",
    "For raising an error, we have an internal function call. From [torch.kthvalue](https://github.com/pytorch/pytorch/blob/14282232d94032e2b6cff2b6cb044e53b95f4196/aten/src/ATen/native/cuda/Sorting.cu#L413):\n",
    "\n",
    "```cpp\n",
    "    at::globalContext().alertNotDeterministic(\"kthvalue CUDA\");\n",
    "```\n",
    "And each operation that has an alternate deterministic implementation must check the flag's state and react appropriately.\n",
    "\n",
    "From [torch.bmm](https://github.com/pytorch/pytorch/blob/14282232d94032e2b6cff2b6cb044e53b95f4196/aten/src/ATen/native/sparse/cuda/SparseCUDATensorMath.cu#L892):\n",
    "\n",
    "```cpp\n",
    "    deterministic = deterministic || globalContext().deterministicAlgorithms();\n",
    "    cusparseSpMMAlg_t mm_alg = deterministic ? CUSPARSE_COOMM_ALG2 : CUSPARSE_COOMM_ALG1;\n",
    "    ...\n",
    "    TORCH_CUDASPARSE_CHECK(cusparseSpMM(..., mm_alg, ...));\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27187286",
   "metadata": {},
   "source": [
    "## Enforcing documentation consistency\n",
    "\n",
    "Documentation for nondeterministic algorithms is not updated automatically, so we're depending on contributors to update the docs for `torch.use_deterministic_algorithms` appropriately any time they change its behavior with respect to any operation.\n",
    "\n",
    "Currently, the solution to this problem is to include a note next to every `at::globalContext().alertNotDeterministic` and `globalContext().deterministicAlgorithms` call in the codebase. The note points to internal documentation explaining the rules for using those API calls.\n",
    "\n",
    "For `at::globalContext().alertNotDeterministic`: [Note [Writing Nondeterministic Operations]](https://github.com/pytorch/pytorch/blob/14282232d94032e2b6cff2b6cb044e53b95f4196/aten/src/ATen/Context.h#L148)\n",
    "\n",
    "For `at::globalContext().deterministicAlgorithms`: [Note [Enabling Deterministic Operations]](https://github.com/pytorch/pytorch/blob/14282232d94032e2b6cff2b6cb044e53b95f4196/aten/src/ATen/Context.h#L119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114b50d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
